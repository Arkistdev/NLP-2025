{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ac4fa1d",
   "metadata": {},
   "source": [
    "Opdracht: Inleveropdracht 4: Machine Translation & Document Search \n",
    "Naam: Onno de Jong <br>\n",
    "Studentnummer: 1809878"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28b610b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7e9849",
   "metadata": {},
   "source": [
    "Embeddings inladen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "735310f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "en, fr = pickle.loads(open('data/en_embeddings.p', 'rb').read()), pickle.loads(open('data/fr_embeddings.p', 'rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55ec470",
   "metadata": {},
   "source": [
    "inladen train en test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c1c50d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/en-fr.train.txt',sep=\" \", names=['en', 'fr'])\n",
    "test =  pd.read_csv('data/en-fr.test.txt',sep=\" \", names=['en', 'fr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8e0c213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8683</th>\n",
       "      <td>demolished</td>\n",
       "      <td>démolie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>agree</td>\n",
       "      <td>acceptez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8148</th>\n",
       "      <td>hebrew</td>\n",
       "      <td>hébreux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7194</th>\n",
       "      <td>opinions</td>\n",
       "      <td>opinions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5494</th>\n",
       "      <td>escape</td>\n",
       "      <td>échapper</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              en        fr\n",
       "8683  demolished   démolie\n",
       "2470       agree  acceptez\n",
       "8148      hebrew   hébreux\n",
       "7194    opinions  opinions\n",
       "5494      escape  échapper"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b1cfc30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>zoo</td>\n",
       "      <td>zoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2562</th>\n",
       "      <td>gibraltar</td>\n",
       "      <td>gibraltar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>electron</td>\n",
       "      <td>électron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>varied</td>\n",
       "      <td>variés</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2609</th>\n",
       "      <td>maya</td>\n",
       "      <td>mayas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             en         fr\n",
       "676         zoo        zoo\n",
       "2562  gibraltar  gibraltar\n",
       "1988   electron   électron\n",
       "1920     varied     variés\n",
       "2609       maya      mayas"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f48731",
   "metadata": {},
   "source": [
    "cleaning van de sets, er zijn een heel aantal worden, vooral bij de franse woorden, die niet in de embeddings zitten, die haal ik er uit omdat ik er geen informatie van weet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a8ed06e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5275938189845475"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([x in fr.keys() for x in train['fr']]) / len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04b0485d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9908020603384842"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([x in en.keys() for x in train['en']]) / len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55a1e33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10872, 2)\n",
      "(5717, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "clean_train = train[train['en'].isin(en.keys())]\n",
    "clean_train = clean_train[clean_train['fr'].isin(fr.keys())]\n",
    "print(clean_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fddd008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2943, 2)\n",
      "(1542, 2)\n"
     ]
    }
   ],
   "source": [
    "print(test.shape)\n",
    "clean_test = test[test['en'].isin(en.keys())]\n",
    "clean_test = clean_test[clean_test['fr'].isin(fr.keys())]\n",
    "print(clean_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a5a4e2",
   "metadata": {},
   "source": [
    "checken of alle woorden nu aanwezig zijn in de embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61b1ba34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([x in fr.keys() for x in clean_train['fr']]) / len(clean_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b65a4fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([x in en.keys() for x in clean_train['en']]) / len(clean_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e295fe7e",
   "metadata": {},
   "source": [
    "Helemaal compleet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f68299e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descrent(X: np.ndarray, Y: np.ndarray, num_iter: int, lr: float)-> tuple[np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Een implementatie van het gradient descent algoritme op de gradient van de frobenius norm\n",
    "    \n",
    "    Params:\n",
    "    --------\n",
    "    X: np.ndarray\n",
    "        Vector waar de Engelse embeddings in staan.\n",
    "    Y: np.ndarray\n",
    "        Vector waar de Franse embeddings in staan.\n",
    "    num_iter: int\n",
    "        Het aantal iteraties dat het algoritme uitvoert.\n",
    "    lr: float\n",
    "        De learning rate van de gradient descent.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    np.ndarray\n",
    "        Het model.\n",
    "    float\n",
    "        De loss aan het einde van het trainen van de weights.\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    R = np.zeros((n,n))\n",
    "    for _ in range(num_iter):\n",
    "        gradient = 2/m * np.dot(X.T, np.dot(X, R) - Y)\n",
    "        R -= gradient * lr\n",
    "    return R, 1/m * ((np.dot(X, R) - Y)**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15b67e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(n, tset):\n",
    "    res = []\n",
    "    for en_word, fr_word in tset[['en', 'fr']].values[:n]:\n",
    "        res.append([en[en_word], fr[fr_word]])\n",
    "        \n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d334cd44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = get_embeddings(len(clean_train), clean_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "003d3e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5717, 300)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ed6c61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, loss = gradient_descrent(t[:, 0], t[:, 1], 400, .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5f5648c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 300)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645f1aee",
   "metadata": {},
   "source": [
    "Cosine distance functie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c21e340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_dist(A : np.ndarray, B : np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Een implementatie van de cosine formule.\n",
    "    \n",
    "    A: np.ndarray\n",
    "        Vector A.\n",
    "    B: np.ndarray\n",
    "        Vector B.\n",
    "    \"\"\"\n",
    "    \n",
    "    # return np.dot(A,B)/(np.linalg.norm(A)*np.linalg.norm(B))\n",
    "    return np.dot (A, B) / (np.sqrt(np.sum(A ** 2)) * np.sqrt(np.sum(B ** 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "060374ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(en_word: str, en_words: dict[str, np.ndarray], fr_words: dict[str, np.ndarray], R: np.ndarray):\n",
    "    \"\"\"\n",
    "    De prediction functie die werkt met het getrainde model.\n",
    "\n",
    "    Params:\n",
    "    --------\n",
    "    en_word: str\n",
    "        Het engelse woord dat vertaald moet worden.\n",
    "    en_words: dict[str, np.ndarray]\n",
    "        De dictionary van embeddings van engelse woorden.\n",
    "    fr_words: dict[str, np.ndarray]\n",
    "        De dictionary van embeddings van franse woorden.\n",
    "    R: np.ndarray\n",
    "        Het model.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    string\n",
    "        het voorspelde franse woord.\n",
    "    \"\"\"\n",
    "    \n",
    "    target = np.matmul(en_words[en_word], R)\n",
    "    closest = [-1, \"\"]\n",
    "    for index, embedding in fr_words.items():\n",
    "        c_dist = cosine_dist(embedding, target)\n",
    "        if c_dist > closest[0]:\n",
    "            closest = [c_dist, index]\n",
    "    return closest[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "083aef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(word_dict: pd.DataFrame, en_words: dict[str, np.ndarray], fr_words: dict[str, np.ndarray], R: np.ndarray):\n",
    "    \"\"\"\n",
    "    Een simpele accuracy functie die de accuracy berekent van een model over een aantal woorden.\n",
    "\n",
    "    Params:\n",
    "    --------\n",
    "    word_dict: pd.DataFrame\n",
    "        Een dataframe met de kolommen \"en\" en \"fr\" die van engels naar frans vertaald moeten worden.\n",
    "    en_words: dict[str, np.ndarray]\n",
    "        De dictionary van embeddings van engelse woorden.\n",
    "    fr_words: dict[str, np.ndarray]\n",
    "        De dictionary van embeddings van franse woorden.\n",
    "    R: np.ndarray\n",
    "        Het model.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        De accuracy van het model.\n",
    "    \"\"\"\n",
    "    \n",
    "    total = 0\n",
    "    count = 0\n",
    "    for en_word, fr_word in word_dict[['en', 'fr']].values:\n",
    "        count += 1\n",
    "        # print(count) if count % 100 == 0 else None\n",
    "        if predict(en_word, en_words, fr_words, R) == fr_word:\n",
    "            total += 1\n",
    "#         print(i, fr_word)\n",
    "    return total / len(word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f105b32",
   "metadata": {},
   "source": [
    "Een ~43% accuracy op de test set en een ~55% accuracy op de train set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1852eb8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43190661478599224"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(clean_test, en, fr, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d65c2a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5460906069616932"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(clean_train, en, fr, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde5fd0b",
   "metadata": {},
   "source": [
    "## Deel II: Document Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "751f86c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'was',\n",
       " 'for',\n",
       " 'that',\n",
       " 'with',\n",
       " 'from',\n",
       " 'this',\n",
       " 'utc',\n",
       " 'his',\n",
       " 'not',\n",
       " 'are',\n",
       " 'talk',\n",
       " 'which',\n",
       " 'also',\n",
       " 'were',\n",
       " 'but',\n",
       " 'have',\n",
       " 'one',\n",
       " 'new',\n",
       " 'first',\n",
       " 'page',\n",
       " 'you',\n",
       " 'they',\n",
       " 'had',\n",
       " 'article',\n",
       " 'who',\n",
       " 'all',\n",
       " 'their',\n",
       " 'there',\n",
       " 'made',\n",
       " 'its',\n",
       " 'people',\n",
       " 'may',\n",
       " 'after',\n",
       " 'other',\n",
       " 'should',\n",
       " 'two',\n",
       " 'score',\n",
       " 'her',\n",
       " 'can',\n",
       " 'would',\n",
       " 'more',\n",
       " 'she',\n",
       " 'when',\n",
       " 'time',\n",
       " 'team',\n",
       " 'american',\n",
       " 'such',\n",
       " 'discussion',\n",
       " 'links']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(en.keys())[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bcb553",
   "metadata": {},
   "source": [
    "Function to clean documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae596231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import regex as re\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "ps = PorterStemmer()\n",
    "regexp = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')\n",
    "\n",
    "with open(\"data/iliad.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    iliad = f.read()\n",
    "with open(\"data/odyssey.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    odyssey = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e7b740d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg eBook of The Iliad\\n    \\nThis ebook is for the use of anyone anywhere in the Un'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iliad[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b3d7d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "badwords = stopwords.words(\"english\") + list(string.punctuation)\n",
    "\n",
    "def clean_document(document):\n",
    "    # verwijzingen geven geen extra informatie\n",
    "    document = re.sub(\"[\\[(].+[\\])]\", '', document)\n",
    "\n",
    "    # Tokenizer, sentences -> words\n",
    "    cleaned_document = regexp.tokenize(document)\n",
    "    cleaned_document_full = \"\"\n",
    "\n",
    "    if len(cleaned_document) > 2: # here we rebuild the cleaned document back to a clean full sentence to show later. This is not used for anything besides showing the sample\n",
    "        cleaned_document_full += cleaned_document[0]\n",
    "\n",
    "        for word in cleaned_document[1:]:\n",
    "            if word not in string.punctuation:\n",
    "                cleaned_document_full += \" \"\n",
    "            \n",
    "            cleaned_document_full += word\n",
    "\n",
    "    # stopwords en punctuation\n",
    "    document = [word for word in cleaned_document if word not in badwords]\n",
    "\n",
    "    # stemming words, Ik stem hier de woorden expliciet niet omdat veel gestemde woorden niet voorkomen in de woord embeddings\n",
    "    # documents = [[ps.stem(x) for x in document] for document in documents]\n",
    "\n",
    "    return document, cleaned_document_full\n",
    "\n",
    "def clean_documents(document: str):\n",
    "    # split documents\n",
    "    documents = re.split(\"[.:;!?]\", document)\n",
    " \n",
    "    return list(zip(*map(clean_document, documents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c93e30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partially cleaned sample:\t To form correct views of individuals we must regard them as forming parts of a great whole —we must measure them by their relation to the mass of beings by whom they are surrounded, and, in contemplating the incidents in their lives or condition which tradition has handed down to us, we must rather consider the general bearing of the whole narrative, than the respective probability of its details\n",
      "Cleaned sample:\t\t\t ['To', 'form', 'correct', 'views', 'individuals', 'must', 'regard', 'forming', 'parts', 'great', 'whole', '—we', 'must', 'measure', 'relation', 'mass', 'beings', 'surrounded', 'contemplating', 'incidents', 'lives', 'condition', 'tradition', 'handed', 'us', 'must', 'rather', 'consider', 'general', 'bearing', 'whole', 'narrative', 'respective', 'probability', 'details']\n",
      "\n",
      "Words there are embeddings for {'condition', 'respective', 'incidents', 'consider', 'relation', 'surrounded', 'details', 'great', 'mass', 'probability', 'correct', 'rather', 'views', 'tradition', 'whole', 'parts', 'measure', 'lives', 'form', 'forming', 'bearing', 'individuals', 'general', 'must', 'narrative'}\n",
      "% amount of words in cleaned sample: 0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "clean_docs, og_docs = {}, {}\n",
    "\n",
    "clean_docs[\"iliad\"], og_docs[\"iliad\"] = clean_documents(iliad)\n",
    "clean_docs[\"odyssey\"], og_docs[\"odyssey\"] = clean_documents(odyssey)\n",
    "\n",
    "\n",
    "print(\"partially cleaned sample:\\t\", og_docs[\"iliad\"][63])\n",
    "print(\"Cleaned sample:\\t\\t\\t\", clean_docs[\"iliad\"][63])\n",
    "actual_words = set(clean_docs[\"iliad\"][63]).intersection(set(en.keys()))\n",
    "print()\n",
    "\n",
    "print(\"Words there are embeddings for\", actual_words)\n",
    "print(\"% amount of words in cleaned sample:\",len(actual_words) / len(clean_docs[\"iliad\"][63]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6262f0",
   "metadata": {},
   "source": [
    "Function to calculate document embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef65872c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_document_embedding(document: list[str]):\n",
    "    return sum([en.get(word, 0) for word in document]) # De embeddings optellen van alle woorden in het document\n",
    "\n",
    "def calculate_document_embeddings(documents: dict[str, list[list[str]]], og_docs):\n",
    "    embeddings = []\n",
    "    docs = {}\n",
    "\n",
    "    index = 0\n",
    "    for key, value in documents.items():\n",
    "        print(key)\n",
    "        for id, document in enumerate(value):\n",
    "            embedding = calculate_document_embedding(document) \n",
    "            if np.any(embedding):\n",
    "                # ik maak 2 aparte lijsten met het document en de embedding omdat geen van beide de key van een dict kan zijn. het id van het document kan worden gebruikt om de embedding op te vragen\n",
    "                embeddings.append(embedding)\n",
    "                docs[index] = [og_docs[key][id], key]\n",
    "                index += 1\n",
    "\n",
    "    return embeddings, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e73ef61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iliad\n",
      "odyssey\n"
     ]
    }
   ],
   "source": [
    "embedding_database, document_database = calculate_document_embeddings(clean_docs, og_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7883bc56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Project Gutenberg eBook of The Iliad This ebook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever',\n",
       " 'iliad']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_database[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4d668f",
   "metadata": {},
   "source": [
    "Nummer 63 is nu een ander document omdat er een aantal documenten voor zaten die waar geen embedding voor beschikbaar was"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ddfe3390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18703"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_database)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15127120",
   "metadata": {},
   "source": [
    "18723 samples, als we ongeveer gemiddeld 10 samples per bucket willen dan hebben we log2(1872) ~= 11 hyperplanes nodig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff90b37",
   "metadata": {},
   "source": [
    "Function to hash documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "abcfbf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_planes(n_planes, n_dimensions):\n",
    "    return np.random.normal(size=[n_dimensions, n_planes])\n",
    "\n",
    "def hash_vector(vec, planes):\n",
    "    dot = np.dot(vec, planes)\n",
    "\n",
    "    sign_of_dot = np.sign(dot)\n",
    "\n",
    "    h = sign_of_dot >= 1\n",
    "\n",
    "    hash_bucket = \"\".join(map(str, map(int, h))) # Translate it to binary representation\n",
    "\n",
    "    return hash_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6366efa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_binary(n):\n",
    "    strings = []\n",
    "    for i in range(2**n):\n",
    "        binary_str = format(i, '0' + str(n) + 'b') # omzetten van int naar binary representatie\n",
    "        strings.append(binary_str)\n",
    "    \n",
    "    return strings\n",
    "\n",
    "def create_buckets(n_planes):\n",
    "    buckets = {}\n",
    "    for x in get_all_binary(n_planes): # De keys van de buckets zijn een representatie van de planes\n",
    "        buckets[x] = []\n",
    "\n",
    "    return buckets\n",
    "\n",
    "def populate_buckets(embedding_database, document_database, planes):\n",
    "    buckets = create_buckets(planes.shape[1])\n",
    "\n",
    "    for id, _ in document_database.items():\n",
    "        buckets[hash_vector(embedding_database[id], planes)].append(id) # hash vector to the correct bucket\n",
    "\n",
    "    return buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f305ecc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = create_planes(11, 300) # Random planes creeren\n",
    "hash_table = populate_buckets(embedding_database, document_database, p) # Hash table bouwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "533ce05a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'00000000000': [1800, 5970, 10901],\n",
       " '00000000001': [4671,\n",
       "  5376,\n",
       "  5842,\n",
       "  8397,\n",
       "  8587,\n",
       "  12403,\n",
       "  12447,\n",
       "  12801,\n",
       "  13576,\n",
       "  13998,\n",
       "  16219,\n",
       "  17264],\n",
       " '00000000010': [1386, 2672, 3864, 7024, 10019, 12477, 12520],\n",
       " '00000000011': [1145,\n",
       "  1394,\n",
       "  2076,\n",
       "  2115,\n",
       "  2142,\n",
       "  2971,\n",
       "  4208,\n",
       "  4258,\n",
       "  4686,\n",
       "  5652,\n",
       "  6455,\n",
       "  6567,\n",
       "  6603,\n",
       "  6866,\n",
       "  7207,\n",
       "  7342,\n",
       "  8044,\n",
       "  9508,\n",
       "  11208,\n",
       "  11919,\n",
       "  13658,\n",
       "  15293,\n",
       "  15440,\n",
       "  15697,\n",
       "  15904,\n",
       "  17867,\n",
       "  18220],\n",
       " '00000000100': [740,\n",
       "  986,\n",
       "  1592,\n",
       "  1734,\n",
       "  3745,\n",
       "  4337,\n",
       "  5085,\n",
       "  6286,\n",
       "  7448,\n",
       "  9885,\n",
       "  9984,\n",
       "  10519,\n",
       "  10864,\n",
       "  10965,\n",
       "  12111,\n",
       "  12716,\n",
       "  13932]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(hash_table.items())[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b5b7d2",
   "metadata": {},
   "source": [
    "Er is geen sprake van een even verdeling van de sample. Dit was ook niet de verwachting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c874cf78",
   "metadata": {},
   "source": [
    "KNN algoritme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ef7d663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(samples, target, n, distance_func=cosine_dist): # Simpele inplementatie van een KNN algoritme (Ik weet niet zeker of ik het helemaal correct heb uitgewerkt op deze manier)\n",
    "    s = [[x, distance_func(target, embedding_database[x])] for x in samples]\n",
    "    s = list(sorted(s, key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    if len(s) > n:\n",
    "        return s[:n]\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5dd564b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence, hash_table, n):\n",
    "    clean_sentence = clean_document(sentence)[0]\n",
    "    embed_sentence = calculate_document_embedding(clean_sentence)\n",
    "\n",
    "    hash = hash_vector(embed_sentence, p)\n",
    "    bucket = [document_database[id] for id in hash_table[hash]]\n",
    "    knn_res = KNN(hash_table[hash], embed_sentence, n)\n",
    "    knn_res = [document_database[x[0]] for x in knn_res] # Convert naar originele samples\n",
    "\n",
    "    books = {}\n",
    "    for doc in knn_res:\n",
    "        if doc[1] in books:\n",
    "            books[doc[1]] += 1\n",
    "            continue\n",
    "        books[doc[1]] = 1\n",
    "    book = sorted(list(books.items()), key=lambda x: x[1])[0][0]\n",
    "\n",
    "    return bucket, knn_res, book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa40962e",
   "metadata": {},
   "source": [
    "Ter voorbeeld dat het allemaal werkt heb ik een passage uit iliad gepakt. Je ziet dat het algoritme de identieke passage vindt en daarbij een aantal andere passages die erop lijken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1b1971d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N samples in bucket: 8\n",
      "Most similar samples: [['and thus the chief of Troy replied', 'iliad'], ['and thus the chief replies', 'iliad'], ['The towering Ajax, with an ample stride, Advanced the first, and thus the chief defied', 'iliad'], ['whom, with fury moved, Opprobrious thus, th ’ impatient chief reproved', 'iliad'], ['” So parts the chief', 'iliad']]\n",
      "This sentence is most similar to: iliad\n"
     ]
    }
   ],
   "source": [
    "s = \"\"\"and thus the chief of Troy replied\"\"\"\n",
    "bucket, knn_res, book = predict(s, hash_table, 5)\n",
    "print(\"N samples in bucket:\", len(bucket))\n",
    "print(\"Most similar samples:\", knn_res)\n",
    "print(\"This sentence is most similar to:\", book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e404ab5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N samples in bucket: 439\n",
      "Most similar samples: [['” “My friend ,” answered Nestor, “you recall a time of much sorrow to my mind, for the brave Achaeans suffered much both at sea, while privateering under Achilles, and when fighting before the great city of king Priam', 'odyssey'], ['he was running up fast into manhood, and bade fare to be no worse man, face and figure, than his father, but some one, either god or man, has been unsettling his mind, so he has gone off to Pylos to try and get news of his father, and the suitors are lying in wait for him as he is coming home, in the hope of leaving the house of Arceisius without a name in Ithaca', 'odyssey'], ['We know what fate befell each one of the other heroes who fought at Troy, but as regards Ulysses heaven has hidden from us the knowledge even that he is dead at all, for no one can certify us in what place he perished, nor say whether he fell in battle on the mainland, or was lost at sea amid the waves of Amphitrite', 'odyssey'], ['” “I do not know ,” answered Medon, “whether some god set him on to it, or whether he went on his own impulse to see if he could find out if his father was dead, or alive and on his way home', 'odyssey'], ['” And Ulysses answered, “In good truth, goddess, it seems I should have come to much the same bad end in my own house as Agamemnon did, if you had not given me such timely information', 'odyssey']]\n",
      "This sentence is most similar to: odyssey\n"
     ]
    }
   ],
   "source": [
    "s = \"\"\"“My friend,” answered Nestor, “you recall a time of much sorrow to my\n",
    "mind, for the brave Achaeans suffered much both at sea, while\n",
    "privateering under Achilles, and when fighting before the great city of king Priam\"\"\"\n",
    "bucket, knn_res, book = predict(s, hash_table, 5)\n",
    "print(\"N samples in bucket:\", len(bucket))\n",
    "print(\"Most similar samples:\", knn_res)\n",
    "print(\"This sentence is most similar to:\", book)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
